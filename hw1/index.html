<!DOCTYPE html>
<html>
    <head>
        <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
        <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
        <style>
            h1 {
                text-align: center;
            }

            .container {
                margin: 0 auto;
                padding: 60px 20%;
            }

            figure {
                text-align: center;
                margin: 20px 0;
            }

            img {
                display: inline-block;
                max-width: 100%;
            }

            body {
                font-family: 'Inter', sans-serif;
                line-height: 1.6;
            }

            .image-grid {
                display: flex;
                flex-wrap: wrap;
                justify-content: center;
                gap: 10px;
            }

            .image-grid figure {
                flex: 1;
                min-width: 300px;
            }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>CS184/284A Spring 2026 Homework 1 Write-Up</h1>
            <div style="text-align: center;">Name: Umesh Gopi</div>

            <br>

            <div style="text-align: center;">
                Link to webpage: <a href="https://cs184.eecs.berkeley.edu/sp26">cs184.eecs.berkeley.edu/sp26</a>
                <br>
                Link to GitHub repository: <a href="https://cs184.eecs.berkeley.edu/sp26">cs184.eecs.berkeley.edu/sp26</a>
            </div>

            <h2>Overview</h2>
            <p>In this homework I built a software rasterizer that takes SVG files and renders them pixel by pixel. I implemented triangle rasterization, supersampling for antialiasing, 2D transforms, barycentric interpolation for color blending, and texture mapping with nearest/bilinear pixel sampling and mipmap-based level sampling. The most interesting takeaway was seeing how mipmaps use UV derivatives to pick the right texture resolution, and how each antialiasing technique (supersampling, bilinear filtering, mipmaps) attacks the problem differently with its own speed/memory/quality tradeoff.</p>

            <h2>Task 1: Drawing Single-Color Triangles</h2>
            <p>To rasterize triangles we first want to find the bounding points of the triangles that we want to sample inside. To do this we get the maximum and minimum of both the x and y points for all 3 given points. After, we want to define the 3 Lines (L(x,y)) that actually make up the triangle so we can use the three Lines Test to properly rasterize our triangles. We can find \(L(x,y) = Ax + By + C\) and solve for A, B and C with some math from the points that have been given to us (specifically, for edge \((x_i,y_i)\) to \((x_j,y_j)\): \(A = -(y_j - y_i)\), \(B = (x_j - x_i)\)). Then we can iterate through all the points inside of our bounding points and see if the center of each pixel falls inside our triangle (i.e. all three line tests have the same sign). If it does then we can color it in and if not we don't.</p>
            <p>It is no worse as we are pretty much doing the same type of sampling inside the bounding of the triangle. The algorithm only iterates over pixels within the bounding box \([\min(x_0,x_1,x_2),\ \max(x_0,x_1,x_2)] \times [\min(y_0,y_1,y_2),\ \max(y_0,y_1,y_2)]\), so no sample outside the bounding box is ever tested.</p>

            <figure>
                <img src="screenshot_2-14_10-18-53.png" alt="Task 1 Screenshot" />
                <figcaption>Screenshot of basic/test4.svg showing the pixel inspector centered on an aliased edge.</figcaption>
            </figure>

            <h2>Task 2: Antialiasing by Supersampling</h2>
            <p>For the SuperSampling implementation, the main change that I did was account for sampling each pixel multiple times instead of just sampling the pixel at the center every time. I updated the buffer data structure to include the sample rate when being used (resized to <code>width × height × sample_rate</code>) to account for more values and hence get better definition when displaying the images at a higher sampling rate. The x and y values are the main things that changed when actually doing the rasterizing apart from updating the actual buffer. We include two loops based on the sampling size (N being defined as the square root of the sampling rate) with the x and y values increasing at increments of 1 over the square root of the sample rate, hence covering much more points than before. All sub-sample colors are then averaged per pixel in <code>resolve_to_framebuffer()</code> before writing to the display.</p>
            <p>This process is very memory intensive but as we are covering more space we see in the pictures below, as we supersample more the edges are more defined and are not crooked/shaking anymore and it becomes more of a straight line. The effect is most noticeable from 1 sample to 4 samples, but still a very minor improvement from 4 samples to 16 samples. I personally would supersample at 4 per pixel as it gives the best balance between memory and actual output image wise.</p>

            <div class="image-grid">
                <figure>
                    <img src="screenshot_2-18_22-50-35.png" alt="Supersampling 1" />
                    <figcaption>Sample rate: 1</figcaption>
                </figure>
                <figure>
                    <img src="screenshot_2-18_22-50-39.png" alt="Supersampling 4" />
                    <figcaption>Sample rate: 4</figcaption>
                </figure>
                <figure>
                    <img src="screenshot_2-18_22-50-44.png" alt="Supersampling 16" />
                    <figcaption>Sample rate: 16</figcaption>
                </figure>
            </div>
            <p>These results are observed because at 1 sample per pixel, each pixel is either fully colored or fully white (no partial coverage). At 4 samples, the 2×2 sub-pixel grid captures partial coverage (e.g. 2 of 4 samples inside = 50% blend), producing smoother edges. At 16 samples, the 4×4 grid gives even finer granularity, though the improvement is subtle since 4 samples already captures most of the perceptible aliasing.</p>

            <h2>Task 3: Transforms</h2>
            <p>In this image the robot will be waving his hand at the user while jumping. I rotated the left arm upward and bent the forearm at an angle to create a waving gesture, and shifted both legs apart slightly to suggest a mid-jump pose.</p>

            <figure>
                <img src="screenshot_2-18_16-10-35.png" alt="Waving Robot" />
                <figcaption>Modified robot.svg with the cubeman in a waving and jumping pose.</figcaption>
            </figure>

            <h2>Task 4: Barycentric coordinates</h2>
            <p>Baycentric Interpolation is based on using the points color at 3 points of the triangle and in a way combining them to form a new color value inside the triangle based on its position. Given a triangle with vertices \(V_0, V_1, V_2\), any point \(P\) can be written as \(P = \alpha \cdot V_0 + \beta \cdot V_1 + \gamma \cdot V_2\) where \(\alpha + \beta + \gamma = 1\). Each weight represents how "close" the point is to the corresponding vertex (a point at \(V_0\) has \(\alpha=1, \beta=0, \gamma=0\); the centroid has all weights equal to \(\frac{1}{3}\)). We do this by calculating alpha, beta and gamma which are derived from the area of the triangle as well as the dx,dy of the lines of the triangle. We use these derived values to interpolate the colors in this equation alpha * color1 + beta * color2 + gamma * color3. The point lies inside the triangle if and only if all three weights are non-negative.</p>

            <div class="image-grid">
                <figure>
                    <img src="screenshot_2-18_16-22-39.png" alt="Barycentric Triangle" />
                    <figcaption>Example of color interpolation in a single triangle. Each vertex is assigned a pure color (red, green, blue), and interior pixels are colored by the barycentric blend, producing the smooth gradient visible here.</figcaption>
                </figure>
                <figure>
                    <img src="screenshot_2-18_16-24-25.png" alt="Test 7 Color Wheel" />
                    <figcaption>Screenshot of basic/test7.svg with sample rate 1.</figcaption>
                </figure>
            </div>

            <h2>Task 5: "Pixel sampling" for texture mapping</h2>
            <p>Pixel sampling is how we figure out the color of each pixel when applying a texture to a triangle. I implemented it by first computing the texture coordinates (using barycentric interpolation of the per-vertex UV values) for each pixel, then fetching the color from the texture. In nearest sampling, I simply pick the color of the closest texel, which is fast but can look blocky or jagged, especially on detailed or magnified textures. In bilinear sampling, I take the four nearest texels and average their colors based on distance (weighted as \((1-s)(1-t) \cdot c_{00} + s(1-t) \cdot c_{10} + (1-s)t \cdot c_{01} + st \cdot c_{11}\), where \(s,t\) are the fractional distances from the sample point to the nearest texel center), which smooths out edges and makes textures look much nicer.</p>
            <p>Comparing screenshots, nearest sampling at 1 sample per pixel looks jagged, while increasing to 16 samples improves it slightly. Bilinear sampling at 1 sample per pixel already looks much smoother, and combining it with 16 samples per pixel produces the clearest, smoothest result. The biggest difference between nearest and bilinear shows up on high-frequency textures or magnified areas, where nearest can look blocky and bilinear keeps edges smooth. When the texture is viewed at roughly 1:1 scale, the two methods produce nearly identical results since sample points land close to texel centers.</p>

            <div class="image-grid">
                <figure>
                    <img src="screenshot_2-18_23-17-9.png" alt="Nearest 1x" />
                    <figcaption>Nearest sampling at 1 sample per pixel.</figcaption>
                </figure>
                <figure>
                    <img src="screenshot_2-18_23-17-29.png" alt="Nearest 16x" />
                    <figcaption>Nearest sampling at 16 samples per pixel.</figcaption>
                </figure>
            </div>
            <div class="image-grid">
                <figure>
                    <img src="screenshot_2-18_23-17-52.png" alt="Bilinear 1x" />
                    <figcaption>Bilinear sampling at 1 sample per pixel.</figcaption>
                </figure>
                <figure>
                    <img src="screenshot_2-18_23-17-57.png" alt="Bilinear 16x" />
                    <figcaption>Bilinear sampling at 16 samples per pixel.</figcaption>
                </figure>
            </div>

            <h2>Task 6: "Level Sampling" with mipmaps for texture mapping</h2>
            <p>Level sampling is a way to choose which mipmap level to use when fetching a texture, based on how much a pixel covers in texture space. I implemented it by calculating how much \((u,v)\) changes when moving one pixel in x and y (the screen-space UV derivatives), scaling by the texture dimensions, and computing the level as \(D = \log_2(\max(L_x, L_y))\). I then select the appropriate mipmap: L_ZERO always uses the base texture, L_NEAREST picks the closest mipmap level (rounding D), and L_LINEAR interpolates between the two adjacent levels. This works together with pixel sampling, so each pixel can use nearest or bilinear interpolation at that mipmap level.</p>
            <p>The tradeoffs between the three techniques: <strong>pixel sampling</strong> (nearest vs. bilinear) uses no extra memory and bilinear only costs ~4× texture reads, but can't fix minification aliasing; <strong>level sampling</strong> (mipmaps) costs ~33% extra memory for the mipmap chain and a small per-pixel computation but dramatically reduces minification aliasing; <strong>supersampling</strong> increases both memory and computation linearly with sample rate (16× rate = 16× cost) but improves all types of aliasing. I rendered four versions using L_ZERO + P_NEAREST, L_ZERO + P_LINEAR, L_NEAREST + P_NEAREST, and L_NEAREST + P_LINEAR. The results clearly show that using L_NEAREST reduces aliasing for distant or small textures, and combining it with bilinear sampling produces the smoothest result.</p>

            <div class="image-grid">
                <figure>
                    <img src="screenshot_2-18_22-36-53.png" alt="L_ZERO P_NEAREST" />
                    <figcaption>L_ZERO and P_NEAREST</figcaption>
                </figure>
                <figure>
                    <img src="screenshot_2-18_22-37-45.png" alt="L_ZERO P_LINEAR" />
                    <figcaption>L_ZERO and P_LINEAR</figcaption>
                </figure>
            </div>
            <div class="image-grid">
                <figure>
                    <img src="screenshot_2-18_22-39-50.png" alt="L_NEAREST P_NEAREST" />
                    <figcaption>L_NEAREST and P_NEAREST</figcaption>
                </figure>
                <figure>
                    <img src="screenshot_2-18_22-38-43.png" alt="L_NEAREST P_LINEAR" />
                    <figcaption>L_NEAREST and P_LINEAR</figcaption>
                </figure>
            </div>
        </div>
    </body>
</html>
